{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = []\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if activation == \"sigmoid\":\n",
    "            self.output = self.sigmoid(self.calculate_net_input()) \n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def calculate_net_input(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            total += self.inputs[i] * self.weights[i]\n",
    "        return total + self.bias\n",
    "\n",
    "    def sigmoid(self, net_input):\n",
    "        return 1 / (1 + math.exp(-net_input))\n",
    "\n",
    "    def relu(self, net_input):\n",
    "        return max(0, net_input)\n",
    "\n",
    "    def least_squares_error(self, target):\n",
    "        return 0.5 * (target - self.output) ** 2\n",
    "    \n",
    "    def l1_error(self, target):\n",
    "        return (target - self.output)\n",
    "\n",
    "    # What's next?\n",
    "    # Need to determine how much the neuron's total input has to change to move closer to the expected output\n",
    "    # What is this value?\n",
    "    # ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ\n",
    "\n",
    "    def derivative_error_wrt_net_input(self, target):\n",
    "        return self.derivative_error_wrt_output(target) * self.derivative_output_wrt_net_input()\n",
    "\n",
    "    # Least Squares Error: 1/2 * (tⱼ - yⱼ)^2\n",
    "    # The partial derivate of the error with respect to actual output then is calculated by:\n",
    "    # ∂E/∂yⱼ = -(tⱼ - yⱼ)\n",
    "    def derivative_error_wrt_output(self, target):\n",
    "        #print(\"derivative_error_wrt_output = \", -1 * (target - self.output))\n",
    "        return -1 * (target - self.output)\n",
    "\n",
    "    # dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)\n",
    "    def derivative_output_wrt_net_input(self):\n",
    "        if activation == \"sigmoid\":\n",
    "            return self.output * (1 - self.output)\n",
    "\n",
    "    # The total net input is the weighted sum of all the inputs to the neuron and their respective weights:\n",
    "    # zⱼ = netⱼ = x₁w₁ + x₂w₂ ...\n",
    "    # The partial derivative of the total net input with respective to a given weight (with everything else held constant) then is:\n",
    "    # = ∂zⱼ/∂wᵢ = some constant + 1 * xᵢw₁^(1-0) + some constant ... = xᵢ\n",
    "    def derivative_net_input_wrt_weight(self, index):\n",
    "        #print(\"derivative_net_input_wrt_weight\", self.inputs[index])\n",
    "        return self.inputs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Creates a Layer in the network consisting of neurons/units.\n",
    "    Each layer has one bias.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    N : Integer\n",
    "        Number of neurons/units in the layer\n",
    "\n",
    "    bias : Float\n",
    "        bias of the layer\n",
    "\n",
    "    neurons : List of objects of class Neuron\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, N, bias):\n",
    "        if bias:\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.bias = random.random()\n",
    "\n",
    "        self.neurons = []\n",
    "\n",
    "        for i in range(N):\n",
    "            self.neurons.append(Neuron(self.bias))\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs\n",
    "    \n",
    "    def inspect(self):\n",
    "        print('Neurons:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(' Neuron', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neurons[n].weights[w])\n",
    "            print('  Bias:', self.bias)\n",
    "            \n",
    "            \n",
    "    def inspect_datapoint(self, inputs):\n",
    "        print('Neurons:', len(self.neurons))\n",
    "        outputs = []\n",
    "        for n in range(len(self.neurons)):\n",
    "            print (' Neuron',n)\n",
    "            op = self.neurons[n].calculate_output(inputs)\n",
    "            outputs.append(op)\n",
    "            print (' Output: ', op)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_bias = None, output_layer_weights = None):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layer = Layer(num_hidden, hidden_layer_bias)\n",
    "        self.output_layer = Layer(num_outputs, output_layer_bias)\n",
    "        # Let's initialize those weights we got there!\n",
    "        self.initialize_weights_hidden_layer(hidden_layer_weights)\n",
    "        self.initialize_weights_output_layer(output_layer_weights)\n",
    "\n",
    "    def inspect(self):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        self.hidden_layer.inspect()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.inspect()\n",
    "        print('------')\n",
    "        \n",
    "    def inspect_datapoint(self, inputs):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        hidden_layer_outputs = self.hidden_layer.inspect_datapoint(inputs)\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        output_layer_outputs = self.output_layer.inspect_datapoint(hidden_layer_outputs)\n",
    "        print('------')     \n",
    "        \n",
    "    def initialize_weights_hidden_layer(self, hidden_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for i in range(self.num_inputs):\n",
    "                if hidden_layer_weights:\n",
    "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
    "                else:\n",
    "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
    "                weight_num += 1\n",
    "\n",
    "\n",
    "    def initialize_weights_output_layer(self, output_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.output_layer.neurons)):\n",
    "            for i in range(len(self.hidden_layer.neurons)):\n",
    "                if output_layer_weights:\n",
    "                    self.output_layer.neurons[h].weights.append(output_layer_weights[weight_num])\n",
    "                else:\n",
    "                    self.output_layer.neurons[h].weights.append(random.random())\n",
    "                weight_num += 1\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
    "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
    "\n",
    "    # Using Stochastic Gradient Descent\n",
    "    # Parameter \"inputs\" is the input to the network.\n",
    "    # Parameter \"target\" is the groundtruth label for the given output.\n",
    "    def train(self, inputs, target, inspect_gradient = False):\n",
    "        self.feed_forward(inputs)\n",
    "\n",
    "        # Step 1: Calculate output neurons derivative \n",
    "        # Calculate ∂E/∂z\n",
    "        derivative_error_wrt_output_layer_net_input = [0] * len(self.output_layer.neurons)\n",
    "        for i in range(len(self.output_layer.neurons)):\n",
    "            #  Calculate ∂E/∂zⱼ\n",
    "            derivative_error_wrt_output_layer_net_input[i] = self.output_layer.neurons[i].derivative_error_wrt_net_input(target[i])\n",
    "\n",
    "        # Step 2: Calculate hidden neurons derivative\n",
    "        # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
    "        derivative_error_wrt_hidden_layer_net_input = [0] * len(self.hidden_layer.neurons)\n",
    "        for i in range(len(self.hidden_layer.neurons)):\n",
    "            temp = 0\n",
    "            for j in range(len(self.output_layer.neurons)):\n",
    "                # what is temp?\n",
    "                # \n",
    "                temp += derivative_error_wrt_output_layer_net_input[j] * self.output_layer.neurons[j].weights[i]\n",
    "\n",
    "            derivative_error_wrt_hidden_layer_net_input[i] = temp * self.hidden_layer.neurons[i].derivative_output_wrt_net_input()\n",
    "\n",
    "        # Step 3: Update output neurons weights\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for weight_num in range(len(self.output_layer.neurons[o].weights)):\n",
    "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "                derivative_error_wrt_weight = derivative_error_wrt_output_layer_net_input[o] * self.output_layer.neurons[o].derivative_net_input_wrt_weight(weight_num)\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.output_layer.neurons[o].weights[weight_num] -= self.learning_rate * derivative_error_wrt_weight\n",
    "                if inspect_gradient:\n",
    "                    print(\" Output Neuron \",o, \"weight \",weight_num, \"gradient: \",derivative_error_wrt_weight, \"del error/net_input\", derivative_error_wrt_output_layer_net_input[o])\n",
    "            self.output_layer.neurons[o].bias -= self.learning_rate * derivative_error_wrt_output_layer_net_input[o]\n",
    "                    \n",
    "\n",
    "        # Step 4: Update hidden neuron weights\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for weight_num in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                derivative_error_wrt_weight = derivative_error_wrt_hidden_layer_net_input[h] * self.hidden_layer.neurons[h].derivative_net_input_wrt_weight(weight_num)\n",
    "                self.hidden_layer.neurons[h].weights[weight_num] -= self.learning_rate * derivative_error_wrt_weight\n",
    "                if inspect_gradient:\n",
    "                    print(\" Hidden Neuron \",h, \"weight \",weight_num, \"gradient: \",derivative_error_wrt_weight, \"del error/net_input\", derivative_error_wrt_hidden_layer_net_input[h])\n",
    "            self.hidden_layer.neurons[h].bias -= self.learning_rate * derivative_error_wrt_hidden_layer_net_input[h]\n",
    "                    \n",
    "                    \n",
    "    def calculate_total_error(self, training_data):\n",
    "        total_error = 0\n",
    "        for t in range(len(training_data)):\n",
    "            training_x, training_output = training_data[t]\n",
    "            self.feed_forward(training_x)\n",
    "\n",
    "            for o in range(len(training_output)):\n",
    "                total_error += self.output_layer.neurons[o].least_squares_error(training_output[o])\n",
    "                #total_error += self.output_layer.neurons[o].l1_error(training_output[o])\n",
    "        return total_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(t,error, color='b'):\n",
    "    plt.plot(t, error, color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
